{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841c2539-e2d8-4caf-9a35-ca80cc6d8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, classification_report\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import os\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d581151-1f97-4d6a-a1ad-498909564969",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"malicious_phish.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "label_mapping = {\n",
    "    'benign': 0,\n",
    "    'phishing': 1,\n",
    "    'defacement': 1,\n",
    "    'malware': 1\n",
    "}\n",
    "\n",
    "df['target'] = df['type'].map(label_mapping)\n",
    "\n",
    "if df['target'].isnull().sum() > 0:\n",
    "    print(\"\\nWarning: Ada label yang tidak dikenali, menghapus baris tersebut...\")\n",
    "    df = df.dropna(subset=['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22e5c7-64e9-455b-a002-813f18bf5798",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3506864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = 'extracted_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633186c5-1f25-4966-ae9c-5847b98e29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(url):\n",
    "    features = {}\n",
    "\n",
    "    # Konversi ke string jaga-jaga kalau ada data bukan string\n",
    "    url = str(url)\n",
    "\n",
    "    # A. Fitur Panjang\n",
    "    features['url_length'] = len(url)\n",
    "    features['hostname_length'] = len(urlparse(url).netloc)\n",
    "    features['path_length'] = len(urlparse(url).path)\n",
    "\n",
    "    # B. Fitur Karakter Spesial\n",
    "    features['count_dot'] = url.count('.')\n",
    "    features['count_hyphen'] = url.count('-')\n",
    "    features['count_at'] = url.count('@')\n",
    "    features['count_question'] = url.count('?')\n",
    "    features['count_percent'] = url.count('%')\n",
    "    features['count_www'] = url.count('www')\n",
    "\n",
    "    # C. Fitur Pola \n",
    "    features['count_digits'] = sum(c.isdigit() for c in url)\n",
    "    features['count_letters'] = sum(c.isalpha() for c in url)\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06340ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cache ditemuakn: 'extracted_features.csv'.\n",
      "Shape Data:  (651191, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_length</th>\n",
       "      <th>hostname_length</th>\n",
       "      <th>path_length</th>\n",
       "      <th>count_dot</th>\n",
       "      <th>count_hyphen</th>\n",
       "      <th>count_at</th>\n",
       "      <th>count_question</th>\n",
       "      <th>count_percent</th>\n",
       "      <th>count_www</th>\n",
       "      <th>count_digits</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>235</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_length  hostname_length  path_length  count_dot  count_hyphen  \\\n",
       "0          16                0           16          2             1   \n",
       "1          35                0           35          2             0   \n",
       "2          31                0           31          2             0   \n",
       "3          88               21           10          3             1   \n",
       "4         235               23           10          2             1   \n",
       "\n",
       "   count_at  count_question  count_percent  count_www  count_digits  \\\n",
       "0         0               0              0          0             0   \n",
       "1         0               0              0          0             1   \n",
       "2         0               0              0          0             1   \n",
       "3         0               1              0          1             7   \n",
       "4         0               1              0          0            22   \n",
       "\n",
       "   count_letters  target  \n",
       "0             13       1  \n",
       "1             29       0  \n",
       "2             25       0  \n",
       "3             63       1  \n",
       "4            199       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(cache_file):\n",
    "    print(f\"File cache ditemuakn: '{cache_file}'.\")\n",
    "    final_df = pd.read_csv(cache_file)\n",
    "else:\n",
    "    print('cache file tidak ditemukan. Ekstraksi...')\n",
    "    feature_df = df['url'].apply(get_features)\n",
    "\n",
    "    final_df = pd.concat([feature_df, df['target']], axis=1)\n",
    "\n",
    "    final_df.to_csv(cache_file, index=False)\n",
    "    print(f\"Ekstraksi Selesai!, data disimpan ke '{cache_file}'\")\n",
    "\n",
    "print(\"Shape Data: \", final_df.shape )\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f5102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    428103\n",
      "1    223088\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492c310-e36a-438b-ae01-24e5503ab257",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5d9614-90a7-4940-b599-eb262aebefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitur, pake drop untuk ngehapus kolom target.\n",
    "X = final_df.drop('target', axis=1)\n",
    "\n",
    "# target dismpan di y, outputnya 0 atau 1\n",
    "y = final_df['target']\n",
    "\n",
    "# bagian train data, pake function train_test_split(), test_size=0.2 itu agar dibagi 80% untuk train dan 20% untuk test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3ca60",
   "metadata": {},
   "source": [
    "Inisiasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95bd5bc6-7aa8-4e10-8b74-b2dd69095c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "models[\"rf\"] = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "models['xgb'] = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656de42a",
   "metadata": {},
   "source": [
    "TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533d5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Ditemukan!: 'model_phishing.pkl'. \n",
      "Load model... \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model_filename = \"model_phishing.pkl\" # xgboost tanpa tfidf\n",
    "\n",
    "final_model = None # xgboost model tanpa tfidf\n",
    "\n",
    "if os.path.exists(model_filename):\n",
    "    print(f\"Model Ditemukan!: '{model_filename}'. \\nLoad model... \")\n",
    "    final_model = joblib.load(model_filename)\n",
    "else:\n",
    "    print(\"model belum ada. Memulai proses training...\")\n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        print(f\"Training model: {name} ... \")\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"   Akurasi: {acc*100:.2f}%\")\n",
    "        print(f\"   Waktu Training: {elapsed:.4f} detik\")\n",
    "        print(\"   Laporan Klasifikasi:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        print(f\"   Deteksi Benar (Phishing tertangkap): {tp}\")\n",
    "        print(f\"   Salah Prediksi (Phishing lolos/False Negative): {fn}  <-- INI YANG BAHAYA\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        final_model = model\n",
    "\n",
    "    joblib.dump(final_model, model_filename)\n",
    "    print(f\"Training Selesai! Model disimpan sebagai '{model_filename}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593db4f",
   "metadata": {},
   "source": [
    "Optimasi Model XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d26fc",
   "metadata": {},
   "source": [
    "TF-IDF Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e4b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file tfidf sudah ada!. Melakukan Load File...\n",
      "load file selesai.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         0\n",
       "2         0\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "651186    1\n",
       "651187    1\n",
       "651188    1\n",
       "651189    1\n",
       "651190    1\n",
       "Name: target, Length: 651191, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "numeric_features = pd.read_csv(cache_file)\n",
    "numeric_without_target = numeric_features.drop('target', axis=1)\n",
    "\n",
    "filename_tfidf = \"tfidf_vectorizer.pkl\" \n",
    "\n",
    "if os.path.exists(filename_tfidf):\n",
    "    print(f\"file tfidf sudah ada!. Melakukan Load File...\")\n",
    "    tfidf = joblib.load(filename_tfidf)\n",
    "    text_features = tfidf.transform(df['url'])\n",
    "    print('load file selesai.')\n",
    "else:\n",
    "    tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=5000)\n",
    "    text_features = tfidf.fit_transform(df['url'])\n",
    "\n",
    "    text_features.shape\n",
    "    joblib.dump(tfidf, filename_tfidf)\n",
    "\n",
    "X_final = hstack([numeric_without_target.astype(float), text_features]) # untuk training pada xgboost yang baru\n",
    "y_final = df['target'] # target untuk xgboost yang baru\n",
    "\n",
    "y_final \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6da48",
   "metadata": {},
   "source": [
    "Split Data Baru (Untuk Optimasi XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4895f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_final, y_final, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d396f",
   "metadata": {},
   "source": [
    "Model XGBoost baru (Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5f184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sudah tersedia di xgboost_phising_98acc.pkl. Melakukan Load...\n",
      "\n",
      "=== HASIL SETELAH UPGRADE (NUMERIC + TEXT) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     85778\n",
      "           1       0.97      0.97      0.97     44461\n",
      "\n",
      "    accuracy                           0.98    130239\n",
      "   macro avg       0.98      0.98      0.98    130239\n",
      "weighted avg       0.98      0.98      0.98    130239\n",
      "\n",
      "False Negative (Bahaya) Sekarang: 1296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename_xgboost = 'xgboost_phising_98acc.pkl' # xgboost baru\n",
    "\n",
    "if os.path.exists(filename_xgboost):\n",
    "    print(f\"Model sudah tersedia di {filename_xgboost}. Melakukan Load...\")\n",
    "    model_xgb_tuned = joblib.load(filename_xgboost)\n",
    "else:\n",
    "    model_xgb_tuned = xgb.XGBClassifier(\n",
    "        n_estimators=500,        \n",
    "        learning_rate=0.05,      \n",
    "        max_depth=10,            \n",
    "        scale_pos_weight=2,      \n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        n_jobs=-1                \n",
    "    )\n",
    "    start_time = time.time()\n",
    "    model_xgb_tuned.fit(X_train_new, y_train_new)\n",
    "    print(f\"Selesai dalam {time.time() - start_time:.2f} detik\")\n",
    "    joblib.dump(model_xgb_tuned, filename_xgboost)\n",
    "\n",
    "\n",
    "y_pred_new = model_xgb_tuned.predict(X_test_new)\n",
    "\n",
    "print(\"\\n=== HASIL SETELAH UPGRADE (NUMERIC + TEXT) ===\")\n",
    "print(classification_report(y_test_new, y_pred_new))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_new, y_pred_new).ravel()\n",
    "print(f\"False Negative (Bahaya) Sekarang: {fn}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b990d720",
   "metadata": {},
   "source": [
    "Model XGB Tuned V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da07fb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model belum ada, Melakukan training\n",
      "Selesai dalam 452.55 detik\n",
      "Hasil dari xgb tuned v2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     85778\n",
      "           1       0.96      0.96      0.96     44461\n",
      "\n",
      "    accuracy                           0.97    130239\n",
      "   macro avg       0.97      0.97      0.97    130239\n",
      "weighted avg       0.97      0.97      0.97    130239\n",
      "\n",
      "False Negative (Bahaya) Sekarang: 1708\n"
     ]
    }
   ],
   "source": [
    "filename_xgboost_V2 = r\"models\\xgboost_tuned_v2.pkl\"\n",
    "\n",
    "if os.path.exists(filename_xgboost_V2):\n",
    "    print(f\"Model sudah tersedia di {filename_xgboost_V2} \\nmelakukan Load... \")\n",
    "    model_xgb_tuned_V2 = joblib.load(filename_xgboost_V2)\n",
    "else:\n",
    "    print(\"Model belum ada, Melakukan training\")\n",
    "    model_xgb_tuned_V2 = xgb.XGBClassifier(\n",
    "        booster=\"gbtree\",\n",
    "        # Boosting strategy\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "\n",
    "        # Tree complexity (PALING PENTING)\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,\n",
    "        gamma=0.1,\n",
    "\n",
    "        # Randomness & generalization (WAJIB utk TF-IDF)\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "\n",
    "        # Regularization (KRUSIAL utk text)\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=1.0,\n",
    "\n",
    "        # Class imbalance (HITUNG, JANGAN TEBAK)\n",
    "        scale_pos_weight= 1.92,\n",
    "\n",
    "        # Performance\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"logloss\",\n",
    "\n",
    "        # Reproducibility & system\n",
    "        random_state=42,\n",
    "        n_jobs=1    \n",
    "    )\n",
    "    start_time = time.time()\n",
    "    model_xgb_tuned_V2.fit(X_train_new, y_train_new)\n",
    "    print(f\"Selesai dalam {time.time() - start_time:.2f} detik\")\n",
    "    joblib.dump(model_xgb_tuned_V2, filename_xgboost_V2)\n",
    "\n",
    "y_pred_new_V2 = model_xgb_tuned_V2.predict(X_test_new)\n",
    "\n",
    "print(\"Hasil dari xgb tuned v2\")\n",
    "print(classification_report(y_test_new, y_pred_new_V2))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_new, y_pred_new_V2).ravel()\n",
    "print(f\"False Negative (Bahaya) Sekarang: {fn}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfcf480",
   "metadata": {},
   "source": [
    "Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eb5b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berhasil memuat 1000000\n"
     ]
    }
   ],
   "source": [
    "df_whitelist = pd.read_csv(r\"dataset\\top-1m.csv\", names=['no','domain'])\n",
    "set_whitelist = set(df_whitelist['domain'].astype(str).values)\n",
    "\n",
    "print(f\"berhasil memuat {len(set_whitelist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f08a0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_phishing(url_input):\n",
    "    \n",
    "    ext = tldextract.extract(url_input)\n",
    "    clean_domain = f\"{ext.domain}.{ext.suffix}\"\n",
    "  \n",
    "\n",
    "\n",
    "    if clean_domain in set_whitelist :\n",
    "        return print(\"Link Aman!\"), print(f\"clean domain: {clean_domain}\")\n",
    "    else:\n",
    "\n",
    "        features_num = get_features(url_input) # fitur untuk nomor\n",
    "        features_num_df = pd.DataFrame([features_num]) # biar jadi dataframe\n",
    "        features_text = tfidf.transform([url_input]) # fitur text\n",
    "        \n",
    "        X_predict = hstack([features_num_df.astype(float), features_text ]) \n",
    "\n",
    "        prediction = model_xgb_tuned_V2.predict(X_predict)[0]\n",
    "        probabilitas = model_xgb_tuned_V2.predict_proba(X_predict)[0][1]\n",
    "        percent = probabilitas * 100\n",
    "\n",
    "        if prediction == 1 :\n",
    "            return print(f\"Link tidak aman!, tingkat bahaya {percent:.2f}%\"), print(prediction), print(f\"clean domain: {clean_domain}\")\n",
    "        else :\n",
    "            return print(f\"Link aman!, tingkat bahaya hanya {percent:.2f}%\"), print(prediction), clean_domain\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf44cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link tidak aman!, tingkat bahaya 99.98%\n",
      "1\n",
      "clean domain: commm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasil = predict_phishing(\"https://www.ladanglima.com\")\n",
    "hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28b7c8",
   "metadata": {},
   "source": [
    "======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec5b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a925f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35903b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
